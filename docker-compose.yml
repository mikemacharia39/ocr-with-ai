# For more on llama.cpp configuration, refer to:
# https://cnb.cool/aigc/llama.cpp/-/blob/0996c5597f680effacc046832bb807c14900e22d/examples/server/README.md
services:
  llama-service:
    image: llama_cpp_custom_image:1.0  # Use the named image
    build:
      context: .  # Points to the directory containing your Dockerfile
      dockerfile: Dockerfile  # Explicitly specifies the Dockerfile
    container_name: llama_cpp_custom
    environment:
      - LLAMA_ARG_MODEL=/models/gemma-3-27b-it-qat-q4_0.gguf
      - LLAMA_ARG_CTX_SIZE=2048
      - LLAMA_ARG_N_PARALLEL=1
    ports:
      - "8080:8080"  # Maps container port 8080 to host port 8001
    volumes:
      - ./models:/models
    networks:
      - llama_network  # Attach to a custom network
    stdin_open: true
    tty: true

# psql -h localhost -p 5432 -U mike temperature_db
# psql -U mike temperature_db
  postgres:
    image: postgres:15
    container_name: postgres_with_pgvector
    environment:
      - POSTGRES_PASSWORD=s3cret
      - POSTGRES_USER=mike
      - POSTGRES_DB=temperature_db
    ports:
      - "5432:5432"
    volumes:
      - ~/apps/postgres:/var/lib/postgresql/data
    command: >
      sh -c "apt-get update &&
             apt-get install -y postgresql-15-pgvector &&
             docker-entrypoint.sh postgres"

# https://collabnix.com/setting-up-ollama-models-with-docker-compose-a-step-by-step-guide/
# Using the Ollama CLI through Docker
# By default, ollama uses mxbai-embed-large model for embeddings.
# docker exec -it ollama ollama pull mxbai-embed-large
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    restart: unless-stopped

  webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui
    ports:
      - "3000:8080"
    depends_on:
      - ollama
    environment:
      - OLLAMA_API_BASE_URL=http://localhost:11434/api
    restart: unless-stopped

volumes:
  ollama_data:
    name: ollama_data

networks:
  llama_network:
    driver: bridge



# Below configuration shows how to run llama.cpp on an actual server
# refer to https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md for docker images for llama.cpp
#  phi:
#    image: ghcr.io/ggml-org/llama.cpp:server-cuda
#    environment:
#      - LLAMA_ARG_N_GPU_LAYERS=999
#      - LLAMA_ARG_MODEL=/models/phi-4.Q4_K_M.gguf
#    ports:
#      - "8000:8080"
#    volumes:
#      - ./models:/models
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [ gpu ]
#  mistral:
#    image: ghcr.io/ggml-org/llama.cpp:server-cuda
#    environment:
#      - LLAMA_ARG_N_GPU_LAYERS=999
#      - LLAMA_ARG_MODEL=/models/mistral-small-3.1-24b-instruct-2503-q8_0.gguf
#    ports:
#      - "8002:8080"
#    volumes:
#      - ./models:/models
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]